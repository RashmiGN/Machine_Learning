{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rqN0gZjqS8hc"
      },
      "source": [
        "\n",
        "# **Analytics 2 :** <font color=#DF4807>**Transformers**</font>\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ijidpwBZKwW4"
      },
      "outputs": [],
      "source": [
        "#load libraries\n",
        "#!pip install transformers datasets torch\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset\n",
        "from datasets import load_metric\n",
        "\n",
        "from transformers import AutoTokenizer\n",
        "from transformers import AutoModelForQuestionAnswering, DistilBertConfig\n",
        "from transformers import Trainer, TrainingArguments\n",
        "\n",
        "from sklearn.metrics import f1_score\n",
        "\n",
        "import torch"
      ],
      "metadata": {
        "id": "cc2fc8DExScE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T9T_XSRhKrvI"
      },
      "source": [
        "##**Load Data**\n",
        "\n",
        "We will be using the TyDiQA (Typologically Diverse Question Answering) dataset from Google research. It includes \"over 200,000 question-answer pairs from 11 languages representing a diverse range of linguistic phenomena and data challenges\". For today we will only be using the English subset of the data. Loading the data will require a few minutes.\n",
        "\n",
        "Further reading: https://ai.google.com/research/tydiqa/dataset\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EvvjxVPSOfMr"
      },
      "outputs": [],
      "source": [
        "train_data = load_dataset('tydiqa', 'primary_task')\n",
        "tydiqa_data = train_data.filter(lambda example: example['language'] == 'english') #extract English only data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hYH0qYt3uPOS",
        "outputId": "3f257189-c74c-4501-a08c-88a6fd8b36b4"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DatasetDict({\n",
              "    train: Dataset({\n",
              "        features: ['passage_answer_candidates', 'question_text', 'document_title', 'language', 'annotations', 'document_plaintext', 'document_url'],\n",
              "        num_rows: 9211\n",
              "    })\n",
              "    validation: Dataset({\n",
              "        features: ['passage_answer_candidates', 'question_text', 'document_title', 'language', 'annotations', 'document_plaintext', 'document_url'],\n",
              "        num_rows: 1031\n",
              "    })\n",
              "})"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ],
      "source": [
        "# view data structure\n",
        "tydiqa_data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oolqWQ4zTwaF"
      },
      "source": [
        "Each example of the data is stored as a dictionary object. Data is stored as questions, contexts and indeces. The indeces show where in the context the answer to the question lies. You can access the index using the `annotations` key. Let's look at a sample question  and answer:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CNJKQyWtTOGm",
        "outputId": "f9c7ead5-4282-472f-fd37-790d099f7855"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Question: What was the lingua franca of the Ottoman Empire?\n",
            "\n",
            "Context: \n",
            "\n",
            "The language of the court and government of the Ottoman Empire was Ottoman Turkish,[1] but many other languages were in contemporary use in parts of the empire. Although the minorities of the Ottoman Empire were free to use their language amongst themselves, if they needed to communicate with the government they had to use Ottoman Turkish.[2]\n",
            "The Ottomans had three influential languages: Turkish, spoken by the majority of the people in Anatolia and by the majority of Muslims of the Balkans except in Albania, Bosnia, and various Aegean Sea isl...\n",
            "\n",
            "Answer: Ottoman Turkish\n"
          ]
        }
      ],
      "source": [
        "index = 700 #changing the index will give you differnet Q&A pairs\n",
        "\n",
        "# starting index\n",
        "start_index = tydiqa_data['train'][index]['annotations']['minimal_answers_start_byte'][0]\n",
        "\n",
        "# ending index\n",
        "end_index = tydiqa_data['train'][index]['annotations']['minimal_answers_end_byte'][0]\n",
        "\n",
        "print(\"Question: \" + tydiqa_data['train'][index]['question_text'])\n",
        "print(\"\\nContext: \"+ tydiqa_data['train'][index]['document_plaintext'][0:550] + '...') # some of the contexts are quite long so we will only view a part of it.\n",
        "print(\"\\nAnswer: \" + tydiqa_data['train'][index]['document_plaintext'][start_index:end_index])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y1068hgGU-oJ"
      },
      "source": [
        "The model we are going to train predicts the start end of where the answer will be in the context. So we need to extract the start and end points from the data. It should be noted that some of the questions do not have answers. In these cases, the start and end indices are set to -1.\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9_Tb49o7VPca",
        "outputId": "fc88f8b8-b76f-438d-df18-903fbbd98948"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'passage_answer_candidate_index': [-1],\n",
              " 'minimal_answers_start_byte': [-1],\n",
              " 'minimal_answers_end_byte': [-1],\n",
              " 'yes_no_answer': ['NONE']}"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ],
      "source": [
        "#lets look at a sample with out an answer.\n",
        "tydiqa_data['train'][0]['annotations']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fs5iucBvv2Qs",
        "outputId": "67f3e5e9-8c84-47ef-c607-770235eb5b06"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'passage_answer_candidate_index': [0],\n",
              " 'minimal_answers_start_byte': [69],\n",
              " 'minimal_answers_end_byte': [84],\n",
              " 'yes_no_answer': ['NONE']}"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ],
      "source": [
        "#lets look at a sample with an answer.\n",
        "tydiqa_data['train'][index]['annotations']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3NNsWZl9VPht"
      },
      "source": [
        "To make the processing a bit easier, we will flatten the data so we don't have to work with a dictionary structure. Flattening the data, will give the data a table struncture."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DUIJGU3mVNET"
      },
      "outputs": [],
      "source": [
        "# Flattening the datasets\n",
        "flattened_train_data = tydiqa_data['train'].flatten().select(range(3500)) #flatten and take a subset of the data\n",
        "flattened_test_data =  tydiqa_data['validation'].flatten().select(range(1000))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2WoDaskbV-16"
      },
      "source": [
        "##**Tokenization**\n",
        "\n",
        "If you plan on using a pretrained model, it's important to use the associated pretrained tokenizer. This ensures the text is split the same way as the pretraining corpus, and uses the same corresponding tokens-to-index (usually referred to as the vocab) during pretraining."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4_xQ-jSxwZcX"
      },
      "outputs": [],
      "source": [
        "# Import the AutoTokenizer from the transformers library\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-cased-distilled-squad\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fR-TZJsdwqYJ"
      },
      "outputs": [],
      "source": [
        "# Processing samples using the 3 steps described.\n",
        "def process_samples(sample):\n",
        "    tokenized_data = tokenizer(sample['document_plaintext'], sample['question_text'], truncation=\"only_first\", padding=\"max_length\")\n",
        "\n",
        "    input_ids = tokenized_data[\"input_ids\"] #sequence of integers that represent the tokens\n",
        "\n",
        "    # We will label impossible answers with the index of the CLS token.\n",
        "    cls_index = input_ids.index(tokenizer.cls_token_id) #cls_token_id is a hugging face attribute that converts \"cls\" to an int\n",
        "\n",
        "    # If no answers are given, set the cls_index as answer.\n",
        "    if sample[\"annotations.minimal_answers_start_byte\"][0] == -1:\n",
        "        start_position = cls_index\n",
        "        end_position = cls_index\n",
        "    else:\n",
        "        # Start/end character index of the answer in the text.\n",
        "        answer_text = sample[\"document_plaintext\"][sample['annotations.minimal_answers_start_byte'][0]:sample['annotations.minimal_answers_end_byte'][0]]\n",
        "        start_char = sample[\"annotations.minimal_answers_start_byte\"][0]\n",
        "        end_char = sample['annotations.minimal_answers_end_byte'][0] #start_char + len(answer_text)\n",
        "\n",
        "        # sometimes answers are off by a character or two\n",
        "        if sample['document_plaintext'][start_char-1:end_char-1] == answer_text:\n",
        "            start_char = start_char - 1\n",
        "            end_char = end_char - 1     # When the answer label is off by one character\n",
        "        elif sample['document_plaintext'][start_char-2:end_char-2] == answer_text:\n",
        "            start_char = start_char - 2\n",
        "            end_char = end_char - 2     # When the answer label is off by two characters\n",
        "\n",
        "        start_token = tokenized_data.char_to_token(start_char)\n",
        "        end_token = tokenized_data.char_to_token(end_char - 1)\n",
        "\n",
        "        # if start position is None, the answer passage has been truncated\n",
        "        if start_token is None:\n",
        "            start_token = tokenizer.model_max_length\n",
        "        if end_token is None:\n",
        "            end_token = tokenizer.model_max_length\n",
        "\n",
        "        start_position = start_token\n",
        "        end_position = end_token\n",
        "\n",
        "    return {'input_ids': tokenized_data['input_ids'],\n",
        "          'attention_mask': tokenized_data['attention_mask'],\n",
        "          'start_positions': start_position,\n",
        "          'end_positions': end_position}\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pPTWn70TwtOW",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 49,
          "referenced_widgets": [
            "e3e5fc42d5e745edaa90ab57e3cd2232",
            "c536a08e9c274686891676f99f0acacd",
            "c62c146132194c0dba4ebc9fe3aaf376",
            "0883648d8fa84006b0c0ee9abc0e6904",
            "f8cf40cf60624d05a048f32ef825db70",
            "1a0787754ba646e9a62e0f65ce00de65",
            "ba22ce29483c4e45bc8af5fda61689bc",
            "1deba09dbe194ffdbaada2f81b9db9a5",
            "af72a87210034a90bc80ec10aec94984",
            "584441957fb94dd59b47e29931b25330",
            "2222e758ef114ac692f3007efee5b461"
          ]
        },
        "outputId": "cf067dab-afe8-4d47-a4c9-3467fd9373a8"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Map:   0%|          | 0/1000 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "e3e5fc42d5e745edaa90ab57e3cd2232"
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "# Tokenizing and processing the flattened dataset\n",
        "processed_train_data = flattened_train_data.map(process_samples)\n",
        "processed_test_data = flattened_test_data.map(process_samples)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3jVX207vUa6F"
      },
      "source": [
        "##**Model**\n",
        "\n",
        "Read Me:  https://huggingface.co/transformers/v3.0.2/model_doc/auto.html\n",
        "\n",
        "Read Me: https://huggingface.co/transformers/v3.0.2/model_doc/auto.html#automodelforquestionanswering\n",
        "\n",
        "The model we will use is the \"distilbert-base-cased-distilled-squad\". The DistilBert model is a smaller and lighter and as a result faster. If you read the documentation here....\n",
        "\n",
        "Link: https://huggingface.co/distilbert-base-cased-distilled-squad#model-details\n",
        "\n",
        "you will see that it is 40% lighter than  bert-base-uncased, runs 60% faster but performs at 95% of BERT's performance (GLUE language understanding benchmark)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2rAALd4wwwlC"
      },
      "outputs": [],
      "source": [
        "# Import the AutoModelForQuestionAnswering for the pre-trained model. We will only fine tune the head of the model\n",
        "model = AutoModelForQuestionAnswering.from_pretrained(\"distilbert-base-cased-distilled-squad\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1ecV6pGDwyPM"
      },
      "outputs": [],
      "source": [
        "columns_to_return = ['input_ids','attention_mask', 'start_positions', 'end_positions'] #attention_mask tells the model which inputs are tokens and which are padding\n",
        "processed_train_data.set_format(type='pt', columns=columns_to_return)\n",
        "processed_test_data.set_format(type='pt', columns=columns_to_return)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CKqtNVVxUh8s"
      },
      "source": [
        "##**Compiling and Fine Tuning**"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#!pip install accelerate -U #setting up distributed training in pytorch"
      ],
      "metadata": {
        "id": "bx0cM7y0w7St"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gf9ecb1gw12e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 235
        },
        "outputId": "4ad149d9-0ab8-4013-ebfe-7eae6b1cc2a3"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='3500' max='3500' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [3500/3500 14:13, Epoch 4/4]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>1.884600</td>\n",
              "      <td>1.816227</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>1.220200</td>\n",
              "      <td>1.907752</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>1.287800</td>\n",
              "      <td>2.081372</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4</td>\n",
              "      <td>1.061700</td>\n",
              "      <td>2.170346</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TrainOutput(global_step=3500, training_loss=1.4096927141462055, metrics={'train_runtime': 856.1443, 'train_samples_per_second': 16.352, 'train_steps_per_second': 4.088, 'total_flos': 1829143400448000.0, 'train_loss': 1.4096927141462055, 'epoch': 4.0})"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ],
      "source": [
        "# Training the model may take around 15 minutes.\n",
        "training_args = TrainingArguments(\n",
        "    output_dir='model_results5',    # output directory where the model predictions and checkpoints will be written.\n",
        "    overwrite_output_dir=True,\n",
        "    evaluation_strategy = \"epoch\",\n",
        "    num_train_epochs=4,             # total number of training epochs\n",
        "    per_device_train_batch_size=4,  # batch size per GPU/TPU core/CPU for training\n",
        "    per_device_eval_batch_size=4,   # batch size per GPU/TPU core/CPU for evaluation\n",
        "    warmup_steps=20,                # number of steps used for a linear warmup from 0 to learning_rate.\n",
        "    learning_rate = 0.00001,        # learning rate\n",
        "    weight_decay=0.0001,            # strength of weight decay (regularisation technique to reduce overfitting)\n",
        "    logging_steps=50,               # how often to log metrics\n",
        "    optim=\"adamw_torch\",            # optimization algorithm\n",
        ")\n",
        "\n",
        "trainer = Trainer(\n",
        "    model = model, # transformer model to be trained\n",
        "    args = training_args, # training arguments, defined above\n",
        "    train_dataset = processed_train_data, # training dataset\n",
        "    eval_dataset = processed_test_data, # evaluation dataset\n",
        ")\n",
        "\n",
        "trainer.train() #will take about 12 minutes"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Note: If you get an error message when running Training arguments, follow these steps:\n",
        "\n",
        "\n",
        "  1. Run pip install accelerate -U in a cell\n",
        "  2. In the top menu click Runtime → Restart Runtime\n",
        "  3. Do not rerun any cells with !pip install in them\n",
        "  4. Rerun all the other code cells and you should be good to go!\n"
      ],
      "metadata": {
        "id": "f8ihMPxixrG_"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "va-0xLi0w3Nf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 124
        },
        "outputId": "b2d27372-fc23-4876-e887-c9a8c6791ad5"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='250' max='250' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [250/250 00:17]\n",
              "    </div>\n",
              "    "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'eval_loss': 2.1703460216522217,\n",
              " 'eval_runtime': 17.7679,\n",
              " 'eval_samples_per_second': 56.281,\n",
              " 'eval_steps_per_second': 14.07,\n",
              " 'epoch': 4.0}"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ],
      "source": [
        "# The evaluation may take around 30 seconds\n",
        "trainer.evaluate(processed_test_data)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z-v6gxg2Up8I"
      },
      "source": [
        "##**Testing**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5GDpVgNyRdEQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "692e785c-bc0a-4cef-cf6e-6134d4875cfc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Question: What has a humid climate?\n",
            "Answer: Heidelberg\n",
            "\n",
            "Question: What is the sunniest month?\n",
            "Answer: July\n",
            "\n",
            "Question: What is the average temperature in April?\n",
            "Answer: 30°C\n",
            "\n",
            "Question: What is the wettest time of year?\n",
            "Answer: [CLS]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "\n",
        "text = r\"\"\"\n",
        "Heidelberg has a humid subtropical climate.\n",
        "The year round warm temperatures are determined by air currents.\n",
        "This results in drier summers and wetter winters.\n",
        "Heidelberg's position in the valley causes more wind than average.\n",
        "Spring starts early and is one of the warmest in Europe.\n",
        "April can be very dry.\n",
        "The rising temperatures in May can create some storms.\n",
        "Nights start cold and stay fresh throughout spring.\n",
        "Day temperatures can become hot from April.\n",
        "The avarege temperature in April is about 30°C.\n",
        "Summers are long, hot and mostly dry.\n",
        "Day temperatures are around 28-30 on average.\n",
        "Temperatures will often rise beyond 35°C in midsummer.\n",
        "July is also the sunniest month of the year.\n",
        "Autumn starts very warm and cools down by the end of November.\n",
        "The region gets affected by fog from the second part of October on.\n",
        "Day temperatures will stay around 20°C until at least mid-October.\n",
        "Nights cool down during October, but remain above 10°C.\n",
        "Winters are mostly mild.\n",
        "Snow is a rare event and it rains often.\n",
        "Winters are the wettest time of the year.\n",
        "Storms can create severe damage.\n",
        "The region is often affected by floods.\n",
        "\"\"\"\n",
        "\n",
        "questions = [\"What has a humid climate?\",\n",
        "             \"What is the sunniest month?\",\n",
        "             \"What is the average temperature in April?\",\n",
        "             \"What is the wettest time of year?\"]\n",
        "\n",
        "for question in questions:\n",
        "    inputs = tokenizer.encode_plus(question, text, return_tensors=\"pt\")\n",
        "    #print(\"inputs\", inputs)\n",
        "    #print(\"inputs\", type(inputs))\n",
        "    input_ids = inputs[\"input_ids\"].tolist()[0]\n",
        "    inputs.to(\"cuda\")\n",
        "\n",
        "    text_tokens = tokenizer.convert_ids_to_tokens(input_ids)\n",
        "    answer_model = model(**inputs)\n",
        "\n",
        "    answer_start = torch.argmax(\n",
        "        answer_model['start_logits']\n",
        "    )  # Get the most likely beginning of answer with the argmax of the score\n",
        "    answer_end = torch.argmax(answer_model['end_logits']) + 1  # Get the most likely end of answer with the argmax of the score\n",
        "\n",
        "    answer = tokenizer.convert_tokens_to_string(tokenizer.convert_ids_to_tokens(input_ids[answer_start:answer_end]))\n",
        "\n",
        "    print(f\"Question: {question}\")\n",
        "    print(f\"Answer: {answer}\\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JUatQp0dXW7F"
      },
      "source": [
        "Note, remember that if you get a CLS token as an answer, the model was not able to find an answer."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Adding Layers to a Pre-Trained Model**\n",
        "\n",
        "ResNet-50 is a convolutional neural network that is 50 layers deep.\n",
        "\n",
        "\n",
        "```\n",
        "from tensorflow.keras import models\n",
        "\n",
        "ResNet = ResNet50(\n",
        "    include_top= None, weights='imagenet', input_tensor=None, input_shape=([128, 217, 3]),\n",
        "    pooling=None, classes=5)\n",
        "model = models.Sequential()\n",
        "model.add(ResNet)\n",
        "model.add(Flatten())\n",
        "model.add(Dense(units=512, activation='relu'))\n",
        "model.add(Dropout(0.5))\n",
        "model.add(Dense(units=256, activation='relu'))\n",
        "model.add(Dropout(0.5))\n",
        "model.add(Dense(units=5, activation='softmax'))\n",
        "\n",
        "```"
      ],
      "metadata": {
        "id": "fSdxTIwd0eXu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Freezing Layers**\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "```\n",
        "# Create a ResNet50 model with pre-trained weights\n",
        "ResNet = ResNet50(\n",
        "    include_top=None, weights='imagenet', input_tensor=None, input_shape=(128, 217, 3),\n",
        "    pooling=None, classes=5)\n",
        "\n",
        "# Set the ResNet layers as untrainable\n",
        "for layer in ResNet.layers:\n",
        "    layer.trainable = False\n",
        "\n",
        "model = models.Sequential()\n",
        "model.add(ResNet)\n",
        "model.add(layers.Flatten())\n",
        "model.add(layers.Dense(units=512, activation='relu'))\n",
        "model.add(layers.Dropout(0.5))\n",
        "model.add(layers.Dense(units=256, activation='relu'))\n",
        "model.add(layers.Dropout(0.5))\n",
        "model.add(layers.Dense(units=5, activation='softmax'))\n",
        "\n",
        "# Compile and train the model\n",
        "```"
      ],
      "metadata": {
        "id": "rUTh1Rwl0hBV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "zoSWxmHH0kIu"
      }
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "toc_visible": true,
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.6"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "e3e5fc42d5e745edaa90ab57e3cd2232": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_c536a08e9c274686891676f99f0acacd",
              "IPY_MODEL_c62c146132194c0dba4ebc9fe3aaf376",
              "IPY_MODEL_0883648d8fa84006b0c0ee9abc0e6904"
            ],
            "layout": "IPY_MODEL_f8cf40cf60624d05a048f32ef825db70"
          }
        },
        "c536a08e9c274686891676f99f0acacd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_1a0787754ba646e9a62e0f65ce00de65",
            "placeholder": "​",
            "style": "IPY_MODEL_ba22ce29483c4e45bc8af5fda61689bc",
            "value": "Map: 100%"
          }
        },
        "c62c146132194c0dba4ebc9fe3aaf376": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_1deba09dbe194ffdbaada2f81b9db9a5",
            "max": 1000,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_af72a87210034a90bc80ec10aec94984",
            "value": 1000
          }
        },
        "0883648d8fa84006b0c0ee9abc0e6904": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_584441957fb94dd59b47e29931b25330",
            "placeholder": "​",
            "style": "IPY_MODEL_2222e758ef114ac692f3007efee5b461",
            "value": " 1000/1000 [00:25&lt;00:00, 13.39 examples/s]"
          }
        },
        "f8cf40cf60624d05a048f32ef825db70": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1a0787754ba646e9a62e0f65ce00de65": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ba22ce29483c4e45bc8af5fda61689bc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "1deba09dbe194ffdbaada2f81b9db9a5": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "af72a87210034a90bc80ec10aec94984": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "584441957fb94dd59b47e29931b25330": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2222e758ef114ac692f3007efee5b461": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}